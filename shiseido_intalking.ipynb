{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse All Comments about Shiseido Products on talking.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function of parsing links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238\n"
     ]
    }
   ],
   "source": [
    "#parse all the links on search result page of talking.com\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "domain_url = \"http://www.intalking.com/\"\n",
    "serp_url = 'http://www.intalking.com/search.php?mod=forum&searchid=17&orderby=lastpost&ascdesc=desc&searchsubmit=yes&kw=%E8%B3%87%E7%94%9F%E5%A0%82'\n",
    "\n",
    "\n",
    "#find all the page links (in h3 tag) \n",
    "def serp_title_link():\n",
    "    h3 = soup.find_all(\"h3\")\n",
    "    for link in h3:\n",
    "        a = link.a.get(\"href\")\n",
    "        whole_a = domain_url + a\n",
    "        url_list.append(whole_a)\n",
    "\n",
    "\n",
    "#next page \n",
    "def next_page_link():\n",
    "    if soup.find('div',class_='pg').find(\"a\", class_=\"nxt\") != None:\n",
    "        #there is no 'next page' in the last page\n",
    "        next_page = soup.find('div',class_='pg').find(\"a\", class_=\"nxt\").get(\"href\")\n",
    "        next_page_url = domain_url + next_page\n",
    "        #print(next_page_url)\n",
    "        return(next_page_url)\n",
    "        \n",
    "\n",
    "url_list = []\n",
    "sleep = 0.1    \n",
    "    \n",
    "\n",
    "while True:\n",
    "    page = requests.get(serp_url).text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    serp_title_link()\n",
    "    next_page_link()\n",
    "    #break when the last page is finished\n",
    "    if next_page_link() == None:\n",
    "        break\n",
    "    serp_url = next_page_link()\n",
    "    time.sleep(sleep)\n",
    "    sleep += 0.01\n",
    "    \n",
    "    \n",
    "print(len(url_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.intalking.com/forum.php?mod=viewthread&tid=254068&highlight=%E8%B3%87%E7%94%9F%E5%A0%82\n",
      "http://www.intalking.com/forum.php?mod=viewthread&tid=204264&highlight=%E8%B3%87%E7%94%9F%E5%A0%82\n",
      "http://www.intalking.com/forum.php?mod=viewthread&tid=189081&highlight=%E8%B3%87%E7%94%9F%E5%A0%82\n",
      "238\n"
     ]
    }
   ],
   "source": [
    "print(url_list[3])\n",
    "print(url_list[33])\n",
    "print(url_list[63])\n",
    "print(len(url_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "for_csv_url_list = []\n",
    "\n",
    "for i in url_list:\n",
    "    list_list = []\n",
    "    list_list.append(i)\n",
    "    for_csv_url_list.append(list_list)\n",
    "    \n",
    "f = open('shiseido_url_talking_com.csv', 'w')\n",
    "w = csv.writer(f)\n",
    "w.writerows(for_csv_url_list)\n",
    "#for i in url_list:\n",
    "#    w.writerows(i)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function of parsing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def page_text(forum_url):\n",
    "    url_inside = forum_url\n",
    "    inside_text = requests.get(url_inside).text\n",
    "    inside_soup = BeautifulSoup(inside_text, \"html.parser\")\n",
    "\n",
    "    #all tds in this page\n",
    "    all_td = inside_soup.find_all('td',class_='t_f')\n",
    "\n",
    "    #td[0] ,split it \n",
    "    if inside_soup.find('td',class_='t_f') != None:\n",
    "        all_td1 = inside_soup.find('td',class_='t_f').text\n",
    "        all_td1_split = all_td1.split(\"\\n\")\n",
    "\n",
    "        length = len(all_td)  #length of all_td\n",
    "        text_list = []  #list for text in this page\n",
    "\n",
    "\n",
    "        #deal with td[0]\n",
    "        unwanted = ['馬上加入美妝IN TALKING 可以看到更多美資訊喔', \n",
    "                    '您需要 登錄 才可以下載或查看，沒有帳號？註冊 ', \n",
    "                    '下載附件',\n",
    "                    'x',\n",
    "                    '\\r']\n",
    "        all_td1_split = list(filter(lambda x : x not in unwanted, all_td1_split))\n",
    "\n",
    "        for word in all_td1_split:\n",
    "            if \".jpg\" not in word and \"保存到相冊\" and \"天前 上傳\" not in word:\n",
    "                text_list.append(word)\n",
    "\n",
    "\n",
    "        #deal with td[1:] \n",
    "        for i in range(1, len(all_td)):\n",
    "            comment_i = all_td[i].text\n",
    "            comment_i_split = comment_i.split(\"\\n\")\n",
    "            for c in comment_i_split:\n",
    "                text_list.append(c)\n",
    "\n",
    "\n",
    "        #strip the text in text_list\n",
    "        new_text_list = []  \n",
    "        for text in text_list:\n",
    "            a = text.strip()\n",
    "            if len(a) != 0:\n",
    "                if \"\\xa0\" in a:\n",
    "                    b = a.replace(\"\\xa0\", \"\")\n",
    "                    new_text_list.append(b)\n",
    "                else:\n",
    "                    new_text_list.append(a)\n",
    "\n",
    "\n",
    "        return(new_text_list)\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        return(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parse all pages!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "text_dict = {}\n",
    "\n",
    "\n",
    "sleep2 = 0.1\n",
    "\n",
    "for url in url_list:\n",
    "    text_dict[url] = page_text(url)\n",
    "    time.sleep(sleep2)\n",
    "    sleep2 += 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "f = open('shiseido_comment_dict.csv', 'w')\n",
    "w = csv.writer(f)\n",
    "#for i in list(text_dict.keys()):\n",
    "#    w.writerows(i)\n",
    "w.writerows(text_dict.items())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
